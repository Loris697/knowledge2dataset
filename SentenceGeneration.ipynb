{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ8MdmXBr8nV",
        "outputId": "2336b366-3695-4c6f-e355-ea1bdf253956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
            "Requirement already satisfied: quanto in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from quanto) (1.11.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate quanto transformers peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AoZ0DlhuLTF",
        "outputId": "fa86bce8-3a1e-48fb-d918-e91729663053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -i https://pypi.org/simple/ bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wYqgM2zIddz4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "from abc import ABC, abstractmethod\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9O4EqpBmr7wz"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(ABC):\n",
        "    \"\"\"\n",
        "    Abstract base class for text generation models.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate(self, prompt: str, max_length: int = 256) -> str:\n",
        "        \"\"\"\n",
        "        Generate text based on the input prompt.\n",
        "\n",
        "        Args:\n",
        "        prompt (str): The input text prompt to generate text from.\n",
        "        max_length (int): The maximum length of the generated text.\n",
        "\n",
        "        Returns:\n",
        "        str: The generated text.\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "DU66XNyXGJ9K"
      },
      "outputs": [],
      "source": [
        "class QuestionGenerator():\n",
        "    def __init__(self,\n",
        "                     model: TextGenerator,\n",
        "                     prompt: str = \"\"\"Ti forniró un oggetto e  una relazione. Genera una domanda. Nella generazione delle domande\n",
        "                     attieniti il piú possibile al oggetto e alla relazione forniti. Produci in output solo la domanda, non la risposta.\n",
        "\n",
        "Oggetto: \"Sagrada Familia\"\n",
        "Relazione: \"architetto\"\n",
        "Domanda: Chi è l'architetto della Sagrada Familia?\n",
        "\n",
        "Oggetto: {object_target}\n",
        "Relazione:{relationship}\n",
        "\"\"\"\n",
        "                     ):\n",
        "      self.model = model\n",
        "      self.prompt = prompt\n",
        "      pass\n",
        "\n",
        "    def generate(self,\n",
        "                 object_target,\n",
        "                 relationship,\n",
        "                 answer1,\n",
        "                 answer2,\n",
        "                 answer3,\n",
        "                 answer4\n",
        "                ):\n",
        "        \"\"\"\n",
        "        Generate text based on the input prompt.\n",
        "\n",
        "        Args:\n",
        "        object_target (str): the subject of the question\n",
        "        relationship (str): the relationshio of the question\n",
        "        answer1 (str): one of the answer\n",
        "        answer2 (str): one of the answer\n",
        "        answer3 (str): one of the answer\n",
        "        answer4 (str): one of the answer\n",
        "\n",
        "        Returns:\n",
        "        str: The final prompt of the LLM\n",
        "        \"\"\"\n",
        "\n",
        "        answers = \"\"\"A) {answer1}\n",
        "B) {answer2}\n",
        "C) {answer3}\n",
        "D) {answer4}\"\"\".format(\n",
        "            answer1=answer1,\n",
        "            answer2=answer2,\n",
        "            answer3=answer3,\n",
        "            answer4=answer4\n",
        "          )\n",
        "\n",
        "        output_model = self.model.generate(self.prompt.format(\n",
        "            object_target=object_target,\n",
        "            relationship=relationship\n",
        "            ))\n",
        "\n",
        "        return  output_model + \"\\n\" +answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vCdi2uu1r7w0"
      },
      "outputs": [],
      "source": [
        "class FaunoModel(TextGenerator):\n",
        "    def __init__(self, device: str = \"cpu\"):\n",
        "        self.tokenizer = LlamaTokenizer.from_pretrained(\"baffo32/decapoda-research-llama-7B-hf\")\n",
        "        self.model = LlamaForCausalLM.from_pretrained(\n",
        "            \"baffo32/decapoda-research-llama-7B-hf\",\n",
        "            load_in_8bit=True,\n",
        "            device_map=device\n",
        "        )\n",
        "        self.model = PeftModel.from_pretrained(self.model, \"andreabac3/Open_Fauno-Italian-LLM-7bB\")\n",
        "        self.model.eval()\n",
        "\n",
        "    def generate(self, question, max_length=256):\n",
        "        prompt = f\"The conversation between human and AI assistant.\\n[|Human|] {question}.\\n[|AI|] \"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        input_ids = inputs[\"input_ids\"].cuda()\n",
        "        generation_output = self.model.generate(\n",
        "            input_ids=input_ids,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=256\n",
        "        )\n",
        "        output = self.tokenizer.decode(generation_output.sequences[0]).split(\"[|AI|]\")[1]\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yyB2Rc3Hr7w0"
      },
      "outputs": [],
      "source": [
        "class LLaMantinoModel(TextGenerator):\n",
        "    def __init__(self,\n",
        "                 device: str = \"cuda\",\n",
        "                 model_id: str = \"swap-uniba/LLaMAntino-2-7b-hf-dolly-ITA\",\n",
        "                 quantization : str = \"float8\"\n",
        "                ):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.tokenizer.add_special_tokens({\"pad_token\":\"<unk>\"})\n",
        "        self.tokenizer.chat_template =   \"{% set ns = namespace(i=0) %}\" \\\n",
        "                                    \"{% for message in messages %}\" \\\n",
        "                                        \"{% if message['role'] == 'user' and ns.i == 0 %}\" \\\n",
        "                                               \"{{ bos_token +' [INST] <<SYS>>\\n' }}\" \\\n",
        "                                               \"{{ 'Sei un assistente disponibile, rispettoso e onesto di nome Llamantino. ' }}\" \\\n",
        "                                               \"{{ 'Rispondi sempre nel modo più utile possibile, pur essendo sicuro. ' }}\" \\\n",
        "                                               \"{{ 'Le risposte non devono includere contenuti dannosi, non etici, razzisti, sessisti, tossici, pericolosi o illegali. ' }}\" \\\n",
        "                                               \"{{ 'Assicurati che le tue risposte siano socialmente imparziali e positive. ' }}\" \\\n",
        "                                               \"{{ 'Se una domanda non ha senso o non è coerente con i fatti, spiegane il motivo invece di rispondere in modo non corretto. ' }}\" \\\n",
        "                                               \"{{ 'Se non conosci la risposta a una domanda, non condividere informazioni false.\\n' }}\" \\\n",
        "                                               \"{{ '<</SYS>>\\n\\n' }}\" \\\n",
        "                                               \"{{ message['content'] + ' [/INST]' }}\" \\\n",
        "                                        \"{% elif message['role'] == 'user' and ns.i != 0 %} \" \\\n",
        "                                            \"{{ bos_token + ' [INST] ' + message['content'] + ' [/INST]' }}\" \\\n",
        "                                        \"{% elif message['role'] == 'assistant' %}\" \\\n",
        "                                            \"{{ ' '  + message['content'] + ' ' + eos_token + ' ' }}\" \\\n",
        "                                        \"{% endif %}\" \\\n",
        "                                        \"{% set ns.i = ns.i+1 %}\" \\\n",
        "                                    \"{% endfor %}\"\n",
        "\n",
        "        #quantization_config = QuantoConfig(weights=quantization)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                                          device_map=device,\n",
        "                                                          load_in_8bit=True,\n",
        "                                                          #quantization_config=quantization_config\n",
        "                                                          )\n",
        "        self.model.eval()\n",
        "\n",
        "        self.pipe = pipeline(model=self.model,\n",
        "            device_map=\"balanced\",\n",
        "            tokenizer=self.tokenizer,\n",
        "            return_full_text=False,  # langchain expects the full text\n",
        "            task='text-generation',\n",
        "            max_new_tokens=512,  # max number of tokens to generate in the output\n",
        "            temperature=0.7 #temperature\n",
        "        )\n",
        "\n",
        "    def generate(self, prompt, max_length=256):\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        text = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "        sequences = self.pipe(text)\n",
        "        output = \"\"\n",
        "        for seq in sequences:\n",
        "            output += output + seq['generated_text']\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bANPnvzlr7w0"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "vnEs-_Y-r7w1",
        "outputId": "b2ed28b4-4bf2-44fd-e675-2f94177e7b00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        Object                           Relationship  \\\n",
              "0                     Adelaide  unità amministrativa in cui è situato   \n",
              "1                      Albania                  relazione diplomatica   \n",
              "2                       Angola                  relazione diplomatica   \n",
              "3                    Australia                  relazione diplomatica   \n",
              "4        Australia Meridionale                                  Paese   \n",
              "..                         ...                                    ...   \n",
              "95  United States Marine Corps                    luogo di fondazione   \n",
              "96                     Vietnam                  relazione diplomatica   \n",
              "97                    Virginia                                  Paese   \n",
              "98                    Virginia                            confina con   \n",
              "99                    Zagabria                        città gemellata   \n",
              "\n",
              "                  Correct   Answer_1               Answer_2  \\\n",
              "0   Australia Meridionale     Taiwan  Australia Meridionale   \n",
              "1                 Francia     Grecia                Francia   \n",
              "2               Australia  Australia                 Canada   \n",
              "3                Birmania  Indonesia              Australia   \n",
              "4               Australia     Grecia              Danimarca   \n",
              "..                    ...        ...                    ...   \n",
              "95             Filadelfia  Indonesia                 Russia   \n",
              "96                Ucraina    Turchia                   Cina   \n",
              "97  Stati Uniti d'America       Iran              Danimarca   \n",
              "98      Carolina del Nord    Turchia               Germania   \n",
              "99               Cracovia  Australia                Messico   \n",
              "\n",
              "                 Answer_3               Answer_4  \n",
              "0                   Mosca                Messico  \n",
              "1                  Canada                Brasile  \n",
              "2                  Italia            Regno Unito  \n",
              "3                Birmania                Georgia  \n",
              "4                 Austria              Australia  \n",
              "..                    ...                    ...  \n",
              "95             Filadelfia                 Taiwan  \n",
              "96                Ucraina                 Canada  \n",
              "97              Indonesia  Stati Uniti d'America  \n",
              "98                 Taiwan      Carolina del Nord  \n",
              "99  Stati Uniti d'America               Cracovia  \n",
              "\n",
              "[100 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ee8c990-1505-4d48-b00a-3c76d5e8011c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Object</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Correct</th>\n",
              "      <th>Answer_1</th>\n",
              "      <th>Answer_2</th>\n",
              "      <th>Answer_3</th>\n",
              "      <th>Answer_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Adelaide</td>\n",
              "      <td>unità amministrativa in cui è situato</td>\n",
              "      <td>Australia Meridionale</td>\n",
              "      <td>Taiwan</td>\n",
              "      <td>Australia Meridionale</td>\n",
              "      <td>Mosca</td>\n",
              "      <td>Messico</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Albania</td>\n",
              "      <td>relazione diplomatica</td>\n",
              "      <td>Francia</td>\n",
              "      <td>Grecia</td>\n",
              "      <td>Francia</td>\n",
              "      <td>Canada</td>\n",
              "      <td>Brasile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Angola</td>\n",
              "      <td>relazione diplomatica</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Canada</td>\n",
              "      <td>Italia</td>\n",
              "      <td>Regno Unito</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Australia</td>\n",
              "      <td>relazione diplomatica</td>\n",
              "      <td>Birmania</td>\n",
              "      <td>Indonesia</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Birmania</td>\n",
              "      <td>Georgia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Australia Meridionale</td>\n",
              "      <td>Paese</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Grecia</td>\n",
              "      <td>Danimarca</td>\n",
              "      <td>Austria</td>\n",
              "      <td>Australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>United States Marine Corps</td>\n",
              "      <td>luogo di fondazione</td>\n",
              "      <td>Filadelfia</td>\n",
              "      <td>Indonesia</td>\n",
              "      <td>Russia</td>\n",
              "      <td>Filadelfia</td>\n",
              "      <td>Taiwan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Vietnam</td>\n",
              "      <td>relazione diplomatica</td>\n",
              "      <td>Ucraina</td>\n",
              "      <td>Turchia</td>\n",
              "      <td>Cina</td>\n",
              "      <td>Ucraina</td>\n",
              "      <td>Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Virginia</td>\n",
              "      <td>Paese</td>\n",
              "      <td>Stati Uniti d'America</td>\n",
              "      <td>Iran</td>\n",
              "      <td>Danimarca</td>\n",
              "      <td>Indonesia</td>\n",
              "      <td>Stati Uniti d'America</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Virginia</td>\n",
              "      <td>confina con</td>\n",
              "      <td>Carolina del Nord</td>\n",
              "      <td>Turchia</td>\n",
              "      <td>Germania</td>\n",
              "      <td>Taiwan</td>\n",
              "      <td>Carolina del Nord</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Zagabria</td>\n",
              "      <td>città gemellata</td>\n",
              "      <td>Cracovia</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Messico</td>\n",
              "      <td>Stati Uniti d'America</td>\n",
              "      <td>Cracovia</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ee8c990-1505-4d48-b00a-3c76d5e8011c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1ee8c990-1505-4d48-b00a-3c76d5e8011c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1ee8c990-1505-4d48-b00a-3c76d5e8011c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-14ed3c8c-af8c-4229-87aa-da936531d4ad\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-14ed3c8c-af8c-4229-87aa-da936531d4ad')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-14ed3c8c-af8c-4229-87aa-da936531d4ad button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"Object\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 76,\n        \"samples\": [\n          \"Australia Meridionale\",\n          \"India\",\n          \"Bielorussia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Relationship\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"considerato essere uguale a\",\n          \"continente\",\n          \"unit\\u00e0 amministrativa in cui \\u00e8 situato\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Correct\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 78,\n        \"samples\": [\n          \"Trinidad e Tobago\",\n          \"Australia Meridionale\",\n          \"Frank\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer_1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 39,\n        \"samples\": [\n          \"Johannesburg\",\n          \"Ungheria\",\n          \"Cuba\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer_2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 38,\n        \"samples\": [\n          \"India\",\n          \"Argentina\",\n          \"Danimarca\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer_3\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 45,\n        \"samples\": [\n          \"Senegal\",\n          \"Cina\",\n          \"Taiwan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer_4\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"Spagna\",\n          \"Francia\",\n          \"Germania\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "dataset = pd.read_csv(\"datasetItaliano.csv\",)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI_FhraBr7w1"
      },
      "source": [
        "# Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ccAjaNy-r7w1"
      },
      "outputs": [],
      "source": [
        "#fauno7b = FaunoModel(device = \"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ccsf_wX-vtbl"
      },
      "outputs": [],
      "source": [
        "#fauno7b.generate(\"Qual'è il significato della vita?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UQZkR_aj6Fhz"
      },
      "outputs": [],
      "source": [
        "#question = \"Ti forniró delle triple composte da 2 oggetti e una relazione. Genera una domanda riguarda la relazione. \" + \"Esempio: oggetto1 = Romolo relazione = Fondatore oggetto2 = Roma\" + \" Domanda: Romolo é il fondatore di Roma?\" + \" oggetto1 = Beaudine relazione=regista oggetto2= 'Road to Paradise'\"\n",
        "\n",
        "#prompt = f\"The conversation between human and AI assistant.\\n[|Human|] {question}.\\n[|AI|] \"\n",
        "#inputs = fauno7b.tokenizer(prompt, return_tensors=\"pt\")\n",
        "#input_ids = inputs[\"input_ids\"].cuda()\n",
        "#generation_output = fauno7b.model.generate(\n",
        "#            input_ids=input_ids,\n",
        "#            return_dict_in_generate=True,\n",
        "#            output_scores=True,\n",
        "#            max_new_tokens=256\n",
        "#        )\n",
        "#fauno7b.tokenizer.decode(generation_output.sequences[0]).split(\"[|AI|]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xGtA1sxy5Wnr"
      },
      "outputs": [],
      "source": [
        "#fauno7b.generate(\"Ti forniró delle triple composte da 2 oggetti e una relazione. Genera una domanda riguarda la relazione.\" +\n",
        "#                      \"Esempio: oggetto1 = Romolo relazione = Fondatore oggetto2 = Roma\" +\n",
        "#                      \"Domanda: Romolo é il fondatore di Roma?\" +\n",
        "#                       \"oggetto1 = Beaudine relazione=regista oggetto2= 'Road to Paradise'\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "e4d8e34cf94f447196d164822ad47760",
            "63bb196495f242cab179a9ad0a7d9764",
            "0849b02ec5df49f8bc2e7bd54eeca28f",
            "e8f57fa0c6bd4c0ab3fe794baca4eb79",
            "3757f3f169cd4d23bb44fc3d1f710a2e",
            "50c07992ae24451d9271a3f984c667ef",
            "b8f811b5b4f44b2281833dd32ca14199",
            "ec918767b8a747e79699ac685dc66191",
            "f66cb05fc3614577932162bae0be9f17",
            "5eb24dc3406a46858d86702d67d6677b",
            "8397de4554fc418db3ff2b91e4797c61"
          ]
        },
        "id": "jZYAG1Grr7w2",
        "outputId": "3486b5b6-7c9e-47de-8b8c-c05b4996c9d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4d8e34cf94f447196d164822ad47760"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_id = \"swap-uniba/LLaMAntino-2-chat-13b-hf-UltraChat-ITA\"\n",
        "llamantino13b = LLaMantinoModel(model_id = model_id, device = \"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "4b5dt_4er7w2",
        "outputId": "1bad2ac2-1341-41cc-e8fb-13334bbd49e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Non ho credenze personali o opinioni. Tuttavia, il significato della vita è un argomento complesso e soggettivo che varia da persona a persona. alcuni possono credere che il significato della vita sia trovare la felicità, raggiungere i propri obiettivi, o contribuire al mondo in qualche modo. altri possono credere che il significato della vita sia trovare un senso di scopo, di connessione, o di significato in un'esperienza spirituale o religiosa. in definitiva, il significato della vita è una questione personale che può essere plasmata da esperienze, valori e credenze individuali.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "llamantino13b.generate(\"Qual'è il significato della vita?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "qKgi4bjG5JGk",
        "outputId": "90d382f7-2d90-47ba-a72b-d2a72fa611f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Sì, come modello di lingua AI, posso formulare domande. Posso anche rispondere a domande e fornire informazioni su vari argomenti.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "llamantino13b.generate(\"Sei in grado di formulare delle domande?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "QUIV2-xA3lRs",
        "outputId": "bdc0bd24-3b47-4b8e-efa9-80c170e288df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Sì, posso fornirvi una triple composta da 2 oggetti e una relazione.\\n\\nOggetto1 = Beaudine relazione = Regista oggetto2 = 'Road to Paradise'\\n\\nLa domanda è: Beaudine è il regista di 'Road to Paradise'?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "llamantino13b.generate(\"Ti forniró delle triple composte da 2 oggetti e una relazione. Genera una domanda riguarda la relazione.\" +\n",
        "                      \" Esempio: oggetto1 = Romolo relazione = Fondatore oggetto2 = Roma\" +\n",
        "                      \" Domanda: Romolo é il fondatore di Roma?\" +\n",
        "                      \"oggetto1 = Beaudine relazione=regista oggetto2= 'Road to Paradise'\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "amWLks1rGNet",
        "outputId": "21ad498f-905b-4816-de13-6813f752c8ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Domanda: Chi ha diretto il film 'Road to Paradise'?\\nA) Beaudine\\nB) Scorsese\\nC) Spielberg\\nD) Nolan\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "llamantino13b.generate(\"\"\"Ti forniró delle triple composte da un oggetto,  una relazione e quattro possibili risposte. Genera una domanda a scelta multipla.\n",
        " Esempio: oggetto1 =  libro '1984'  relazione = autore risposta1= Huxley risposta2 = Orwell risposta3 = Hemingway risposta4 = Bradbury\n",
        " Domanda: Chi ha scritto il libro '1984'?\n",
        "A) Huxley\n",
        "B) Orwell\n",
        "C) Hemingway\n",
        "D) Bradbury\n",
        "\n",
        " oggetto1 = 'Road to Paradise' relazione=regista risposta1= Beaudine risposta2 = Scorsese risposta3 =  Spielberg risposta4 = Nolan\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "cPif8oMhMOTd",
        "outputId": "a327769c-bb3d-479e-999b-fe81f4e9adf1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Domanda: Quale paese ha il confine più lungo con la Germania?\\nA) Irlanda\\nB) Belgio\\nC) Portogallo\\nD) Bulgaria'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "questionGenerator = QuestionGenerator(model = llamantino13b)\n",
        "\n",
        "question = questionGenerator.generate(\n",
        "            object_target=\"Germania\",\n",
        "            relationship=\"confine\",\n",
        "            answer1=\"Irlanda\",\n",
        "            answer2=\"Belgio\",\n",
        "            answer3=\"Portogallo\",\n",
        "            answer4=\"Bulgaria\"\n",
        ")\n",
        "\n",
        "question"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the output file\n",
        "output_file = 'questions.txt'\n",
        "\n",
        "# Check if file exists, if not, create it by opening and closing it immediately\n",
        "if not os.path.exists(output_file):\n",
        "    open(output_file, 'w').close()"
      ],
      "metadata": {
        "id": "_2zeIti0a6QD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, append data to the file\n",
        "with open(output_file, 'a', encoding='utf-8') as file:\n",
        "    for index, row in tqdm(dataset.iterrows(), total=dataset.shape[0], desc=\"Writing rows\"):\n",
        "      #Formatting the data into a string\n",
        "      question = questionGenerator.generate(\n",
        "            object_target=row[\"Object\"],\n",
        "            relationship=row[\"Relationship\"],\n",
        "            answer1=row[\"Answer_1\"],\n",
        "            answer2=row[\"Answer_2\"],\n",
        "            answer3=row[\"Answer_3\"],\n",
        "            answer4=row[\"Answer_4\"]\n",
        "      )\n",
        "\n",
        "      # Append the result to the file\n",
        "      file.write(question + '\\tCorrect:'+ row[\"Correct\"] + '\\n')\n",
        "      #print(question + '\\t'+ row[\"Correct\"] + '\\n')"
      ],
      "metadata": {
        "id": "idiv7RwdTvh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "876aa1b1-1aa9-4b78-bafc-8f4935283208"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Writing rows: 100%|██████████| 100/100 [09:34<00:00,  5.75s/it]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e4d8e34cf94f447196d164822ad47760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63bb196495f242cab179a9ad0a7d9764",
              "IPY_MODEL_0849b02ec5df49f8bc2e7bd54eeca28f",
              "IPY_MODEL_e8f57fa0c6bd4c0ab3fe794baca4eb79"
            ],
            "layout": "IPY_MODEL_3757f3f169cd4d23bb44fc3d1f710a2e"
          }
        },
        "63bb196495f242cab179a9ad0a7d9764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50c07992ae24451d9271a3f984c667ef",
            "placeholder": "​",
            "style": "IPY_MODEL_b8f811b5b4f44b2281833dd32ca14199",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0849b02ec5df49f8bc2e7bd54eeca28f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec918767b8a747e79699ac685dc66191",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f66cb05fc3614577932162bae0be9f17",
            "value": 3
          }
        },
        "e8f57fa0c6bd4c0ab3fe794baca4eb79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5eb24dc3406a46858d86702d67d6677b",
            "placeholder": "​",
            "style": "IPY_MODEL_8397de4554fc418db3ff2b91e4797c61",
            "value": " 3/3 [01:55&lt;00:00, 36.33s/it]"
          }
        },
        "3757f3f169cd4d23bb44fc3d1f710a2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50c07992ae24451d9271a3f984c667ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8f811b5b4f44b2281833dd32ca14199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec918767b8a747e79699ac685dc66191": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f66cb05fc3614577932162bae0be9f17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5eb24dc3406a46858d86702d67d6677b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8397de4554fc418db3ff2b91e4797c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}