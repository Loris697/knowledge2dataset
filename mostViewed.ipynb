{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Pacchetti"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZE70Dc5EdjSK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "425e98fc-7693-4f49-a8f8-313647178f21",
    "ExecuteTime": {
     "end_time": "2024-05-27T22:46:19.939177Z",
     "start_time": "2024-05-27T22:46:19.936469Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variabili globali"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Variabili globali\n",
    "WIKIPEDIA_API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "nentities = 10000\n",
    "BATCH_SIZE = 50"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T22:46:19.948529Z",
     "start_time": "2024-05-27T22:46:19.946529Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Moduli"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def process_batch(titles_batch, api_url=\"https://en.wikipedia.org/w/api.php\"):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": \"|\".join(titles_batch),\n",
    "        \"prop\": \"pageprops\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(api_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        pages = response.json().get('query', {}).get('pages', {})\n",
    "        batch_entities = {}\n",
    "        for page_id, page_info in pages.items():\n",
    "            pageprops = page_info.get('pageprops', {})\n",
    "            wikidata_id = pageprops.get('wikibase_item')\n",
    "            if wikidata_id:\n",
    "                batch_entities[page_info['title']] = wikidata_id\n",
    "        return batch_entities\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "        return {}  # Return an empty dict in case of failure\n",
    "    \n",
    "def fetch_wikidata_ids(most_viewed_pages, batch_size=50, api_url=\"https://en.wikipedia.org/w/api.php\"):\n",
    "    entities = {}\n",
    "    titles_batch = []\n",
    "\n",
    "    for page in most_viewed_pages:\n",
    "        titles_batch.append(page['article'])\n",
    "        if len(titles_batch) >= batch_size:\n",
    "            entities.update(process_batch(titles_batch, api_url))\n",
    "            titles_batch = []  # Reset the batch\n",
    "\n",
    "    if titles_batch:  # Process any remaining titles\n",
    "        entities.update(process_batch(titles_batch, api_url))\n",
    "    return entities\n",
    "\n",
    "def fetch_most_viewed_pages(total_pages, api_url=\"https://en.wikipedia.org/w/api.php\"):\n",
    "    limit = 500\n",
    "    fetched_pages = []\n",
    "\n",
    "    for i in range(0, total_pages, limit):\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"list\": \"mostviewed\",\n",
    "            \"pvimlimit\": min(limit, total_pages - i)\n",
    "        }\n",
    "\n",
    "        response = requests.get(api_url, params=params)\n",
    "        response_data = response.json()\n",
    "\n",
    "        if 'query' in response_data and 'mostviewed' in response_data['query']:\n",
    "            fetched_pages.extend(response_data['query']['mostviewed'])\n",
    "        else:\n",
    "            break  # Exit loop if no more data is available\n",
    "\n",
    "    return fetched_pages\n",
    "\n",
    "def get_relations_batch(wikidata_ids, batch_size=50, api_url=\"https://www.wikidata.org/w/api.php\"):\n",
    "    relations = {}\n",
    "    for start in range(0, len(wikidata_ids), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = wikidata_ids[start:end]\n",
    "        wikidata_ids_str = \"|\".join(batch)\n",
    "\n",
    "        print(f\"Processing batch {start // batch_size + 1} out of {len(wikidata_ids) // batch_size + 1}\")\n",
    "\n",
    "        params = {\n",
    "            \"action\": \"wbgetentities\",\n",
    "            \"format\": \"json\",\n",
    "            \"ids\": wikidata_ids_str,\n",
    "            \"props\": \"claims\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(api_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Received status code {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            data = response.json().get('entities', {})\n",
    "        except ValueError:\n",
    "            print(\"Error: Unable to parse JSON response\")\n",
    "            continue\n",
    "\n",
    "        for wikidata_id in batch:\n",
    "            if wikidata_id in data:\n",
    "                claims = data[wikidata_id].get('claims', {})\n",
    "                relations[wikidata_id] = claims\n",
    "            else:\n",
    "                print(f\"Warning: No data found for Wikidata ID {wikidata_id}\")\n",
    "\n",
    "    return relations\n",
    "\n",
    "def query(request):\n",
    "    lastContinue = {}\n",
    "    while True:\n",
    "        # Clone original request\n",
    "        req = request.copy()\n",
    "        # Modify it with the values returned in the 'continue' section of the last result.\n",
    "        req.update(lastContinue)\n",
    "\n",
    "        user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "        result = requests.get('https://wikimedia.org/api/rest_v1/metrics/pageviews/top/en.wikipedia/all-access/2024/01/01', headers={'User-Agent': user_agent}).json()\n",
    "\n",
    "        if 'error' in result:\n",
    "            raise Exception(result['error'])\n",
    "        if 'warnings' in result:\n",
    "            print(result['warnings'])\n",
    "        if 'items' in result:\n",
    "            yield result['items'][0]['articles']  # The articles are in 'items' -> first element -> 'articles'\n",
    "        if 'continue' not in result:\n",
    "            break\n",
    "        lastContinue = result['continue']\n",
    "\n",
    "def most_viewed():\n",
    "    request = {}\n",
    "\n",
    "    res = query(request)\n",
    "    nl = list()\n",
    "    for r in res:\n",
    "        nl.extend(r)\n",
    "\n",
    "    return nl"
   ],
   "metadata": {
    "id": "mhqmn2j9EwTU",
    "ExecuteTime": {
     "end_time": "2024-05-27T22:46:19.966470Z",
     "start_time": "2024-05-27T22:46:19.957222Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def query(date):\n",
    "    url = f'https://wikimedia.org/api/rest_v1/metrics/pageviews/top/en.wikipedia/all-access/{date}'\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "    result = requests.get(url, headers={'User-Agent': user_agent}).json()\n",
    "\n",
    "    if 'error' in result:\n",
    "        raise Exception(result['error'])\n",
    "    if 'warnings' in result:\n",
    "        print(result['warnings'])\n",
    "    if 'items' in result:\n",
    "        return result['items'][0]['articles']\n",
    "    return []\n",
    "\n",
    "def most_viewed(n):\n",
    "    unique_articles = {}\n",
    "    date = datetime.strptime(\"2024-01-01\", \"%Y-%m-%d\")\n",
    "    \n",
    "    while len(unique_articles) < n:\n",
    "        formatted_date = date.strftime(\"%Y/%m/%d\")\n",
    "        articles = query(formatted_date)\n",
    "        \n",
    "        for article in articles:\n",
    "            unique_articles[article['article']] = article\n",
    "        \n",
    "        date -= timedelta(days=1)  # Passa al giorno precedente\n",
    "\n",
    "    # Prendi solo i primi n articoli\n",
    "    most_viewed_articles = list(unique_articles.values())[:n]\n",
    "    \n",
    "    return most_viewed_articles\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T22:46:19.971Z",
     "start_time": "2024-05-27T22:46:19.967733Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calcolo paginie più visitate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 10000 pages\n"
     ]
    }
   ],
   "source": [
    "# Esempio di utilizzo\n",
    "n = 10000  # Numero di pagine uniche richieste\n",
    "most_viewed_pages = most_viewed(n)\n",
    "print(f\"Fetched {len(most_viewed_pages)} pages\")\n",
    "\n",
    "entities = fetch_wikidata_ids(most_viewed_pages)\n",
    "entities_list = [str(element) for element in list(entities.values())]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T22:47:55.523312Z",
     "start_time": "2024-05-27T22:46:19.971973Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 out of 193\n",
      "Processing batch 2 out of 193\n",
      "Processing batch 3 out of 193\n",
      "Processing batch 4 out of 193\n",
      "Processing batch 5 out of 193\n",
      "Processing batch 6 out of 193\n",
      "Processing batch 7 out of 193\n",
      "Processing batch 8 out of 193\n",
      "Processing batch 9 out of 193\n",
      "Processing batch 10 out of 193\n",
      "Processing batch 11 out of 193\n",
      "Processing batch 12 out of 193\n",
      "Processing batch 13 out of 193\n",
      "Processing batch 14 out of 193\n",
      "Processing batch 15 out of 193\n",
      "Processing batch 16 out of 193\n",
      "Processing batch 17 out of 193\n",
      "Processing batch 18 out of 193\n",
      "Processing batch 19 out of 193\n",
      "Processing batch 20 out of 193\n",
      "Processing batch 21 out of 193\n",
      "Processing batch 22 out of 193\n",
      "Processing batch 23 out of 193\n",
      "Processing batch 24 out of 193\n",
      "Processing batch 25 out of 193\n",
      "Processing batch 26 out of 193\n",
      "Processing batch 27 out of 193\n",
      "Processing batch 28 out of 193\n",
      "Processing batch 29 out of 193\n",
      "Processing batch 30 out of 193\n",
      "Processing batch 31 out of 193\n",
      "Processing batch 32 out of 193\n",
      "Processing batch 33 out of 193\n",
      "Processing batch 34 out of 193\n",
      "Processing batch 35 out of 193\n",
      "Processing batch 36 out of 193\n",
      "Processing batch 37 out of 193\n",
      "Processing batch 38 out of 193\n",
      "Processing batch 39 out of 193\n",
      "Processing batch 40 out of 193\n",
      "Processing batch 41 out of 193\n",
      "Processing batch 42 out of 193\n",
      "Processing batch 43 out of 193\n",
      "Processing batch 44 out of 193\n",
      "Processing batch 45 out of 193\n",
      "Processing batch 46 out of 193\n",
      "Processing batch 47 out of 193\n",
      "Processing batch 48 out of 193\n",
      "Processing batch 49 out of 193\n",
      "Processing batch 50 out of 193\n",
      "Processing batch 51 out of 193\n",
      "Processing batch 52 out of 193\n",
      "Processing batch 53 out of 193\n",
      "Processing batch 54 out of 193\n",
      "Processing batch 55 out of 193\n",
      "Processing batch 56 out of 193\n",
      "Processing batch 57 out of 193\n",
      "Processing batch 58 out of 193\n",
      "Processing batch 59 out of 193\n",
      "Processing batch 60 out of 193\n",
      "Processing batch 61 out of 193\n",
      "Processing batch 62 out of 193\n",
      "Processing batch 63 out of 193\n",
      "Processing batch 64 out of 193\n",
      "Processing batch 65 out of 193\n",
      "Processing batch 66 out of 193\n",
      "Processing batch 67 out of 193\n",
      "Processing batch 68 out of 193\n",
      "Processing batch 69 out of 193\n",
      "Processing batch 70 out of 193\n",
      "Processing batch 71 out of 193\n",
      "Processing batch 72 out of 193\n",
      "Processing batch 73 out of 193\n",
      "Processing batch 74 out of 193\n",
      "Processing batch 75 out of 193\n",
      "Processing batch 76 out of 193\n",
      "Processing batch 77 out of 193\n",
      "Processing batch 78 out of 193\n",
      "Processing batch 79 out of 193\n",
      "Processing batch 80 out of 193\n",
      "Processing batch 81 out of 193\n",
      "Processing batch 82 out of 193\n",
      "Processing batch 83 out of 193\n",
      "Processing batch 84 out of 193\n",
      "Processing batch 85 out of 193\n",
      "Processing batch 86 out of 193\n",
      "Processing batch 87 out of 193\n",
      "Processing batch 88 out of 193\n",
      "Processing batch 89 out of 193\n",
      "Processing batch 90 out of 193\n",
      "Processing batch 91 out of 193\n",
      "Processing batch 92 out of 193\n",
      "Processing batch 93 out of 193\n",
      "Processing batch 94 out of 193\n",
      "Processing batch 95 out of 193\n",
      "Processing batch 96 out of 193\n",
      "Processing batch 97 out of 193\n",
      "Processing batch 98 out of 193\n",
      "Processing batch 99 out of 193\n",
      "Processing batch 100 out of 193\n",
      "Processing batch 101 out of 193\n",
      "Processing batch 102 out of 193\n",
      "Processing batch 103 out of 193\n",
      "Processing batch 104 out of 193\n",
      "Processing batch 105 out of 193\n",
      "Processing batch 106 out of 193\n",
      "Processing batch 107 out of 193\n",
      "Processing batch 108 out of 193\n",
      "Processing batch 109 out of 193\n",
      "Processing batch 110 out of 193\n",
      "Processing batch 111 out of 193\n",
      "Processing batch 112 out of 193\n",
      "Processing batch 113 out of 193\n",
      "Processing batch 114 out of 193\n",
      "Processing batch 115 out of 193\n",
      "Processing batch 116 out of 193\n",
      "Processing batch 117 out of 193\n",
      "Processing batch 118 out of 193\n",
      "Processing batch 119 out of 193\n",
      "Processing batch 120 out of 193\n",
      "Processing batch 121 out of 193\n",
      "Processing batch 122 out of 193\n",
      "Processing batch 123 out of 193\n",
      "Processing batch 124 out of 193\n",
      "Processing batch 125 out of 193\n",
      "Processing batch 126 out of 193\n",
      "Processing batch 127 out of 193\n",
      "Processing batch 128 out of 193\n",
      "Processing batch 129 out of 193\n",
      "Processing batch 130 out of 193\n",
      "Processing batch 131 out of 193\n",
      "Processing batch 132 out of 193\n",
      "Processing batch 133 out of 193\n",
      "Processing batch 134 out of 193\n",
      "Processing batch 135 out of 193\n"
     ]
    }
   ],
   "source": [
    "relations = get_relations_batch(entities_list)\n",
    "print(\"Processing complete.\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-27T22:47:55.526765Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mT7q-T1tzLd",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Print out the relations\n",
    "limit = 1\n",
    "count = 0\n",
    "for wikidata_id, claims in relations.items():\n",
    "    if count<=limit:\n",
    "        count+=1\n",
    "        print(f\"Entity: {wikidata_id}\")\n",
    "        for property_id, claim_list in claims.items():\n",
    "            print(f\"  Property: {property_id}\")\n",
    "            for claim in claim_list:\n",
    "                mainsnak = claim['mainsnak']\n",
    "                if 'datavalue' in mainsnak:\n",
    "                    value = mainsnak['datavalue']\n",
    "                    print(f\"    Value: {value}\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ED1sFI0Cxn3b",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "triples = []\n",
    "\n",
    "for wikidata_id, claims in relations.items():\n",
    "    for property_id, claim_list in claims.items():\n",
    "        for claim in claim_list:\n",
    "            mainsnak = claim['mainsnak']\n",
    "            if 'datavalue' in mainsnak:\n",
    "                value = mainsnak['datavalue']\n",
    "                if value['type'] == 'wikibase-entityid':\n",
    "                    value_id = value['value']['id']\n",
    "                    #print(f\"Entity: {wikidata_id} Relation: {property_id} Entity: {value_id}\")\n",
    "                    triples.append({'entity': wikidata_id, 'rel': property_id, 'objt': value_id})\n",
    "\n",
    "# Crea il DataFrame dalle triple\n",
    "df = pd.DataFrame(triples)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIrZHeiV2PgI",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(f\"Numero di entità (head) presenti: {df.entity.nunique()}\")\n",
    "print(f\"Numero di relazioni presenti: {df.rel.nunique()}\")\n",
    "print(f\"Numero di entità (tail) presenti: {df.objt.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eWHmi56r6Zk",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/df_triple.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
